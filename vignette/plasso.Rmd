---
title: "An introduction to plasso"
author:
  - Michael Knaus
  - Stefan Glaisner
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: assets/plasso_refs.bib
link-citations: true
output:
  rmarkdown::html_vignette:
    fig_width: 8
    fig_height: 6
    toc: true
    code_folding: show
vignette: >
  %\VignetteIndexEntry{An introduction to plasso}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, include=FALSE}

hook_output <- knitr::knit_hooks$get("output")

knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x <- xfun::split_lines(x)
    if (length(x) > n) {
      x <- c(head(x, n), "....\n")
    }
    x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})

```

This notebook provides a detailed overview over the `plasso` package and its two main functions `plasso` and `cv.plasso` which were developed in the course of @knaus. This package is strongly oriented around the `glmnet` package and rests on its standard function `glmnet` in its very basis. Related theory and algorithms are described in @glmnet.


# Getting started

Make sure that you install the package from its [Github page](https://github.com/stefan-1997/plasso). For the installation you will need the `devtools` package. General dependencies are: `glmnet`, `Matrix`, `methods`, `parallel`, `doParallel`, `foreach` and `iterators`.

```{r startup, eval=FALSE, error=FALSE, warning=FALSE, message=FALSE}

library(devtools)
devtools::install_github("stefan-1997/plasso")

```

Then, `plasso` can be loaded via the standard `library` function.

```{r load}

library(plasso)

```

The package generally provides two functions `plasso` and `cv.plasso` which are both built on top of the `glmnet` functionality. Specifically, a `glmnet` object lives within both functions and also in their outputs (list item `lasso_full`).

The term `plasso` refers to a Post-Lasso model which estimates a least squares algorithm only for the active (i.e. non-zero) coefficients of a previously estimated Lasso models. This follows the idea that we want to do selection but without shrinkage.

The package comes with some simulated data representing the following DGP:

The covariates matrix $X$ consists of 25 variables whose effect size one the target $Y$ is defined by the vector $\boldsymbol{\pi} = [1, 0.9, 0.8, ..., 0.1, 0, ..., 0]'$ where the first 10 effects decrease from 1 to 0.1 by 0.1 whereas the remaining ones are zero. The variables in $X$ follow a normal distribution with mean zero while the covariance matrix follows a Toeplitz matrix, which is characterized by having constant diagonals:
$$
\boldsymbol{\Sigma} = \begin{bmatrix} 1 & 0.7 & 0.7^2 & ... & 0.7^{24} \\ 0.7 & 1 & 0.7 & ... & 0.7^{23} \\ 0.7^2 & 0.7 & 1 & ... & 0.7^{22} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0.7^{24} & 0.7^{23} & 0.7^{22} & ... & 1 \end{bmatrix}
$$

The target $\boldsymbol{y}$ is then a linear transformation of $\boldsymbol{X}$ plus a vector of standard normal random variables. Each element of $\boldsymbol{y}$ is given by:
$$
y_i = \boldsymbol{X}_i \boldsymbol{\pi} + \varepsilon_i
$$
where $\varepsilon_i \sim \mathcal{N}(0,4)$.

```{r data}

data(toeplitz)

y = as.matrix(toeplitz[,1])
X = toeplitz[,-1]

```

# plasso

`plasso` basically estimates least squares estimates for all lambda values of a standard `glmnet` object. The function's standard output is comparable to the one of a simple `glmnet` output.

```{r fitplasso, out.lines=10}

p = plasso::plasso(X,y)
p

```

You can plot the coefficient paths for both the Post-Lasso model as well as the underlying 'original' Lasso model.

```{r plotplasso, error=FALSE, warning=FALSE, message=FALSE}

plot(p, lasso=FALSE, xvar="lambda")

plot(p, lasso=TRUE, xvar="lambda")

```

We can also have a look at which coefficients are active for a chosen lambda value. Here, the difference between Post-Lasso and Lasso becomes clearly visible. For the Lasso model, there is not only feature selection but shrinkage which results in the active coefficients being smaller than for the Post-Lasso model:

```{r coefplasso, error=FALSE, warning=FALSE, message=FALSE}

coef_p = coef(p, s=0.01)

as.vector(coef_p$plasso)
as.vector(coef_p$lasso)

```


# cv.plasso

The `cv.plasso` function uses cross-validation to determine the performance of different values for the `lambda` penalty term for both models (Post-Lasso and Lasso). The returned output of class `cv.plasso` includes the mean squared errors.

When applying the `summary` method and setting the `default` parameter as FALSE, we get some informative output considering the optimal choice of lambda.

```{r fitcvplasso}

p.cv = plasso::cv.plasso(X,y,kf=5,parallel=FALSE)
summary(p.cv, default=FALSE)

```

Plot the cross-validated MSE by Lambda value for both models.

```{r plotcvplasso, fig.width=7, fig.height=3}

plot(p.cv, legend_pos="left", legend_size=0.5)

```

Plot the cross-validated MSE by Lambda value for both models.

```{r plotcvglmnet, fig.width=7, fig.height=3}

glm.cv = glmnet::cv.glmnet(X,y)

plot(glm.cv)

```

We can use the following code to get the value of the optimal `lambda` (for the Post-Lasso model) and the coefficients at that value of $\lambda$.

```{r index_min_plasso}

p.cv$lambda_min_pl

```

```{r coef_min_plasso, out.lines=10}

coef_pcv = coef(p.cv, S="optimal")
as.vector(coef_pcv$plasso)

```

<br>
<br>